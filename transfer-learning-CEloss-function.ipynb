{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\nimport torch\nimport time\nimport os\nimport numpy as np\nfrom pathlib import Path\nfrom PIL import Image\nfrom skimage.transform import resize\nimport helper\nimport matplotlib.pyplot as plt\nfrom matplotlib import pyplot\nfrom matplotlib.image import imread\nfrom torchvision import datasets\nfrom torchvision import datasets, transforms, models\nfrom torch import nn, optim, Tensor\nimport torch.nn.functional as F\nimport torch.nn as nn\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom keras.preprocessing.image import load_img\nfrom keras.preprocessing.image import img_to_array\nfrom torch.utils.data import DataLoader, Dataset, TensorDataset\nimport random\nseed=42","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('Using device:', device)\nprint()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_path = Path('../input/95cloud-cloud-segmentation-on-satellite-images/95-cloud_training_only_additional_to38-cloud')\nred_dir   = base_path/'train_red_additional_to38cloud'\nblue_dir  = base_path/'train_blue_additional_to38cloud'\ngreen_dir = base_path/'train_green_additional_to38cloud'\nnir_dir   = base_path/'train_nir_additional_to38cloud'\ngt_dir    = base_path/'train_gt_additional_to38cloud'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class RGB_CloudDataset (Dataset):\n    def __init__(self, red_dir, blue_dir, green_dir, gt_dir, transform= None):\n        \n        \n        self.transform   = transform\n    \n        \n        # Listing subdirectories\n        # Loop through the files in red folder  \n        # and combine, into a dictionary, the other bands\n        \n        self.files = [self.combine_files(f, green_dir, blue_dir, gt_dir) \n                      for f in red_dir.iterdir() if not f.is_dir()]\n        \n        random.seed (seed)\n        self.files = random.sample (self.files, k= 8000)\n        \n        \n    def combine_files(self, red_file: Path, green_dir, blue_dir, gt_dir):\n        \n        files = {'red': red_file, \n                 'green':green_dir/red_file.name.replace('red', 'green'),\n                 'blue': blue_dir/red_file.name.replace('red', 'blue'), \n                 'gt': gt_dir/red_file.name.replace('red', 'gt')}\n\n        return files\n    \n\n    \n    \n    def OpenAsArray(self, idx):\n        \n        TrueColor = np.stack([np.array(Image.open(self.files[idx]['red'])),\n                              np.array(Image.open(self.files[idx]['green'])),\n                              np.array(Image.open(self.files[idx]['blue']))], axis = 2)\n     \n    \n    \n        TrueColor = TrueColor.transpose((2, 0, 1))\n    \n        return TrueColor \n    \n    \n    \n    def OpenMask(self, idx, add_dims=False):\n        raw_mask=np.array(Image.open(self.files[idx]['gt']))\n        raw_mask = np.where(raw_mask==255, 1, 0)\n        \n        return np.expand_dims(raw_mask, 0) if add_dims else raw_mask\n\n\n        \n    def __len__(self):\n        return len(self.files)\n    \n    \n    def __getitem__(self, idx):\n        x = self.OpenAsArray(idx)\n        y = self.OpenMask(idx, add_dims=False)\n        \n        \n        if self.transform is not None:\n            x, y = self.transform((x, y))\n        \n        \n        return torch.from_numpy(x), torch.from_numpy(y)\n    \n    \n    def open_as_pil(self, idx):\n        arr = self.OpenAsArray(idx)\n        \n        return Image.fromarray(arr.astype(np.uint8), 'RGB')  \n    \n    \n    \n    def __repr__(self):\n        s = 'Dataset class with {} files'.format(self.__len__())\n\n        return s","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class NirGB_CloudDataset (Dataset):\n    def __init__(self, red_dir, blue_dir, green_dir, nir_dir, gt_dir, transform = None):\n        \n        \n        self.transform = transform\n        \n        # Listing subdirectories\n        # Loop through the files in red folder  \n        # and combine, into a dictionary, the other bands\n        \n        self.files = [self.combine_files(f, green_dir, blue_dir, nir_dir, gt_dir) \n                      for f in red_dir.iterdir() if not f.is_dir()]\n        \n        \n        \n    def combine_files(self, red_file: Path, green_dir, blue_dir, nir_dir, gt_dir):\n        \n        files = {'red': red_file, \n                 'green':green_dir/red_file.name.replace('red', 'green'),\n                 'blue': blue_dir/red_file.name.replace('red', 'blue'), \n                 'nir': nir_dir/red_file.name.replace('red', 'nir'),\n                 'gt': gt_dir/red_file.name.replace('red', 'gt')}\n\n        return files\n    \n    \n    \n    def OpenAsArray(self, idx):\n        \n        FalseColor = np.stack([np.array(Image.open(self.files[idx]['nir'])),\n                               np.array(Image.open(self.files[idx]['green'])),\n                               np.array(Image.open(self.files[idx]['blue']))], axis = 2)\n     \n                    \n        FalseColor      = FalseColor.transpose((2, 0, 1))\n    \n        return (FalseColor / np.iinfo(FalseColor.dtype).max)\n    \n    \n    \n    def OpenMask(self, idx, add_dims=False):\n        raw_mask=np.array(Image.open(self.files[idx]['gt']))\n        raw_mask = np.where(raw_mask==255, 1, 0)\n        \n        return np.expand_dims(raw_mask, 0) if add_dims else raw_mask\n\n\n        \n    def __len__(self):\n        return len(self.files)\n    \n    \n    def __getitem__(self, idx):\n        x = self.OpenAsArray(idx)\n        y = self.OpenMask(idx, add_dims=False)\n        return torch.from_numpy(x), torch.from_numpy(y)\n    \n    \n    def open_as_pil(self, idx):\n        arr = 256 * self.OpenAsArray(idx)\n        return Image.fromarray(arr.astype(np.uint8), 'NirGB')  \n    \n    \n    \n    def __repr__(self):\n        s = 'Dataset class with {} files'.format(self.__len__())\n        return s","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Resize(object):\n    def __init__(self, size = 256):\n        self.size = size\n    def __call__(self, sample):\n        x, y = sample\n        return (resize(x, (x.shape[0], self.size, self.size), mode = \"constant\", \n                      preserve_range = True, anti_aliasing = False),\n                resize(y, (self.size, self.size), mode = \"constant\", \n                      preserve_range = True, anti_aliasing = False))\n    \n\nclass Normalize(object):\n    def __init__(self, mean, std):\n        self.mean = mean\n        self.std  = std\n        \n    def __call__(self, sample):\n        x,y =sample\n        x = x.transpose(1,2,0)\n        x=(x-self.mean)/self.std\n        return x.transpose(2,0,1), y\n    \n    \n# TODO: Define transforms for the training data and testing data\ntrain_transforms=transforms.Compose([Resize(256),\n                                     Normalize([0.485, 0.456, 0.406], [0.229, 0.224,0.225])])\n\n                                 \ntest_transforms=transforms.Compose(Normalize([0.485, 0.456, 0.406], [0.229, 0.224,0.225]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"RGB_data   = RGB_CloudDataset(red_dir, blue_dir, green_dir, gt_dir, transform = train_transforms) \nNirGB_data = NirGB_CloudDataset(red_dir, blue_dir, green_dir, nir_dir, gt_dir, transform = None)\nRGB_x, RGB_y = RGB_data[1000]\nNirGB_x, NirGB_y = NirGB_data[1000]\nRGB_x.shape, RGB_y.shape, NirGB_x.shape, NirGB_y.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# splitting the data into train, validation, and test datasets\n\nRGBtrain_size = int(0.75 * len(RGB_data))\nRGBvalid_size = int(0.15 * len(RGB_data))\nRGBtest_size  = len(RGB_data) - RGBtrain_size - RGBvalid_size\nRGBremaining_size = len(RGB_data) - RGBtrain_size \n\nRGBtrain_dataset, RGBremaining_dataset = torch.utils.data.random_split(RGB_data, \n                                                                       [RGBtrain_size, RGBremaining_size])\nRGBvalid_dataset, RGBtest_dataset      = torch.utils.data.random_split(RGBremaining_dataset, \n                                                                       [RGBvalid_size, RGBtest_size])\n\n\nprint('\\t\\t\\tDataset')\nprint(\"Train data: \\t\\t{}\".format(len(RGBtrain_dataset)),\n      \"\\nValidation data: \\t{}\".format(len(RGBvalid_dataset)),\n     \"\\nTest data: \\t\\t{}\".format(len(RGBtest_dataset)))\n\n\n\nRGBtrain_loader = DataLoader(RGBtrain_dataset, batch_size=12, shuffle=True, num_workers=2)\nRGBvalid_loader = DataLoader(RGBvalid_dataset, batch_size=12, shuffle=True, num_workers=2)\nRGBtest_loader  = DataLoader(RGBtest_dataset , batch_size=12, shuffle=True, num_workers=2)\n\nRGBdata_iter = iter(RGBvalid_loader)\nrgb_img, mask = next(RGBdata_iter)\n\nprint('\\n')\nprint('Raw RGB image shape on batch size = {}'.format(rgb_img.size()))\nprint('Cloud Mask shape on batch size    = {}'.format(mask.size()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Model: Pretrained Vgg16**","metadata":{}},{"cell_type":"code","source":"class Vgg16(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # Use a pretrained model\n        network = models.vgg16(pretrained=True)\n        # Replace the classifier\n        self.modified_network = nn.Sequential(*list((*list(network.children())[:-2],\n                                                     nn.Conv2d(512,2, kernel_size = 1),\n                                                     nn.Upsample(size=(256,256), mode='bilinear', align_corners=False)))) \n    \n    \n    \n    def forward(self, xb):\n        return self.modified_network(xb)\n    \n    def freeze(self):\n        # To freeze the CONV layers\n        for param in self.modified_network[0].parameters():\n            param.require_grad = False\n        for param in self.modified_network[1:].parameters():\n            param.require_grad = True\n    \n    def unfreeze(self):\n        # Unfreeze all layers\n        for param in self.modified_network.parameters():\n            param.require_grad = True","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Vgg16().to(device)\nmodel.modified_network [0], model.modified_network [1:]","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#checking if we have the correct dimensions for our (3,384,384) images\n#summary(Vgg16, (3, 384, 384))   \n\n# checking if the network works, using one batch of the training set\nrgb_img, mask = next(iter(RGBtrain_loader))\nrgb_img, mask = rgb_img.to(device, dtype=torch.float), mask.to(device) \noutput = model (rgb_img).to(device)\noutput.shape, mask.shape ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loss function\nloss_fn = nn.CrossEntropyLoss()\n\n# Set up cutom optimizer with weight decay\noptimizer = optim.Adam(model.parameters(), lr=0.0001)\n\n\ndef acc_fn(predb, yb):\n    predb = torch.sigmoid(predb)\n    return (predb.argmax(dim=1) == yb.to(device)).float().mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(model, train_dl, valid_dl, loss_fn, optimizer, acc_fn, epochs, save_path):\n    \n    start = time.time()\n    min_valid_epoch_loss = np.inf\n    best_acc = 0.0\n    train_losses, valid_losses = [], []\n    train_accuracies, valid_accuracies = [], []\n\n    for e in range(epochs):\n        print('Epoch {}'.format(e))\n        print('-' * 10)\n\n        for phase in ['train', 'valid']:\n            if phase == 'train':\n                model.train()  # Set model to train mode\n                dataloader = train_dl\n            else:\n                model.eval()  # Set model to evaluate mode\n                dataloader = valid_dl\n\n            running_loss = 0.0\n            running_acc  = 0.0\n\n\n            # iterate over data\n            for x, y in dataloader:\n                x = x.to(device, dtype=torch.float)\n                y = y.to(device, dtype=torch.int64)\n\n                # forward pass \n                if phase == 'train':\n                    # zero the gradients\n                    optimizer.zero_grad()\n                    outputs = model(x)\n                    loss    = loss_fn(outputs, y)\n                    # backward + optimize only if in training phase\n                    # the backward pass frees the graph memory, so there is no \n                    # need for torch.no_grad in this training pass\n                    loss.backward()\n                    optimizer.step()\n                    # scheduler.step()\n\n                else:\n                    with torch.no_grad():\n                        outputs = model(x)\n                        loss    = loss_fn(outputs, y.long())\n\n                # stats - whatever is the phase\n                acc = acc_fn(outputs, y)\n                running_acc  += acc * dataloader.batch_size\n                running_loss += loss.item() * dataloader.batch_size\n                    \n            epoch_loss = running_loss / len(dataloader.dataset)\n            epoch_acc  = running_acc / len(dataloader.dataset)\n\n            print('{} \\tLoss: {:.4f} \\tAcc: {}'.format( phase, epoch_loss, epoch_acc))\n            print()\n\n            train_losses.append(epoch_loss), train_accuracies.append(epoch_acc) if phase=='train' else valid_losses.append(epoch_loss), valid_accuracies.append(epoch_acc)\n\n             # save model if validation loss has decreased\n            if phase == 'valid':\n                if epoch_loss <= min_valid_epoch_loss:\n                    print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n                        min_valid_epoch_loss, epoch_loss)) \n                    print('Best Validation Accuracy ({:.6f} --> {:.6f}).'.format(\n                        best_acc, epoch_acc))\n                    torch.save(model.state_dict(), save_path)\n                    min_valid_epoch_loss = epoch_loss\n                    best_acc = epoch_acc\n            \n\n    time_elapsed = time.time() - start\n    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))    \n    \n    return train_losses, valid_losses, train_accuracies, valid_accuracies, best_acc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_accuracies(train_accuracies, valid_accuracies):\n    plt.plot(train_accuracies, '-bx')\n    plt.plot(valid_accuracies, '-rx')\n    plt.xlabel('epoch')\n    plt.ylabel('accuracy')\n    plt.title('Accuracy vs. No. of epochs');\n    \ndef plot_losses(train_losses, valid_losses):\n    plt.plot(train_losses, '-bx')\n    plt.plot(valid_losses, '-rx')\n    plt.xlabel('epoch')\n    plt.ylabel('loss')\n    plt.legend(['Training', 'Validation'])\n    plt.title('Loss vs. No. of epochs');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.freeze()\nPretrained_Vgg16 = './Pretrained_Vgg16_CloudSegModel.pt'\ntrain_losses, valid_losses, train_accuracies, valid_accuracies, best_acc = train(model,RGBtrain_loader, RGBvalid_loader, loss_fn, optimizer, acc_fn, epochs=25, save_path = Pretrained_Vgg16)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_accuracies(train_accuracies, valid_accuracies)\nplt.savefig('./Accuracies_plot_Pretrained_Vgg16.png')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_losses(train_losses, valid_losses)\nplt.savefig('./Losses_plot_Pretrained_Vgg16.png')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Model: Untrained Vgg16**","metadata":{}},{"cell_type":"code","source":"class Vgg16_np(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # Use a pretrained model\n        network = models.vgg16(pretrained=False)\n        # Replace the classifier\n        self.modified_network = nn.Sequential(*list((*list(network.children())[:-2],\n                                                     nn.Conv2d(512,2, kernel_size = 1),\n                                                     nn.Upsample(size=(256,256), mode='bilinear', align_corners=False)))) \n    \n    \n    \n    def forward(self, xb):\n        return self.modified_network(xb)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_np = Vgg16_np().to(device)\nmodel_np.modified_network [0], model_np.modified_network [1:]","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loss function\nloss_fn = nn.CrossEntropyLoss()\n\n# Set up cutom optimizer with weight decay\noptimizer = optim.Adam(model_np.parameters(), lr=0.0001)\n\n\ndef acc_fn(predb, yb):\n    predb = torch.sigmoid(predb)\n    return (predb.argmax(dim=1) == yb.to(device)).float().mean()\n\nUntrained_Vgg16 = './Untrained_Vgg16_CloudSegModel.pt'\ntrain_losses, valid_losses, train_accuracies, valid_accuracies, best_acc= train(model_np, RGBtrain_loader, RGBvalid_loader, \n                                             loss_fn, optimizer, acc_fn, epochs=25, save_path = Untrained_Vgg16)","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_accuracies(train_accuracies, valid_accuracies)\nplt.savefig('./Accuracies_plot_Untrained_Vgg16.png')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_losses(train_losses, valid_losses)\nplt.savefig('./Losses_plot_Untrained_Vgg16.png')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Model: Pretrained Resnet34**","metadata":{}},{"cell_type":"code","source":"class ResNet34(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # Use a pretrained model\n        network = models.resnet34(pretrained=True)\n        # Replace the classifier\n        self.modified_network = nn.Sequential(*list((*list(network.children())[:-2],\n                                                     nn.Conv2d(512,2, kernel_size = 1),\n                                                     nn.Upsample(size=(256,256), mode='bilinear', align_corners=False)))) \n    \n    \n    \n    def forward(self, xb):\n        return self.modified_network(xb)\n    \n    def freeze(self):\n        # To freeze the CONV layers\n        for param in self.modified_network[:8].parameters():\n            param.require_grad = False\n        for param in self.modified_network[8].parameters():\n            param.require_grad = True\n        for param in self.modified_network[9].parameters():\n            param.require_grad = True\n    \n    def unfreeze(self):\n        # Unfreeze all layers\n        for param in self.modified_network.parameters():\n            param.require_grad = True","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ResNet_model = ResNet34().to(device)\nResNet_model.modified_network[8], ResNet_model.modified_network[9]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking if the network works, using one batch of the training set\nrgb_img, mask = next(iter(RGBtrain_loader))\nrgb_img, mask = rgb_img.to(device, dtype=torch.float), mask.to(device) \noutput = ResNet_model (rgb_img).to(device)\noutput.shape, mask.shape ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loss function\nloss_fn = nn.CrossEntropyLoss()\n\n# Set up cutom optimizer with weight decay\noptimizer = optim.Adam(ResNet_model.parameters(), lr=0.0001)\n\n\ndef acc_fn(predb, yb):\n    predb = torch.sigmoid(predb)\n    return (predb.argmax(dim=1) == yb.to(device)).float().mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ResNet_model.freeze()\nPretrained_ResNet34 = './Pretrained_ResNet34_CloudSegModel.pt'\ntrain_losses, valid_losses, train_accuracies, valid_accuracies, best_acc = train(ResNet_model,RGBtrain_loader, RGBvalid_loader, loss_fn, optimizer, acc_fn, epochs=25, save_path = Pretrained_ResNet34)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_accuracies(train_accuracies, valid_accuracies)\nplt.savefig('./Accuracies_plot_Pretrained_ResNet34.png')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_losses(train_losses, valid_losses)\nplt.savefig('./Losses_plot_Pretrained_ResNet34.png')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Model: Untrained Resnet34**","metadata":{}},{"cell_type":"code","source":"class ResNet34_np(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # Use a pretrained model\n        self.network = models.resnet34(pretrained=False)\n        # Replace the classifier\n        self.network = nn.Sequential(*list((*list(self.network.children())[:-2],\n                                                     nn.Conv2d(512,2, kernel_size = 1),\n                                                     nn.Upsample(size=(256,256), mode='bilinear', align_corners=False)))) \n    \n    \n    \n    def forward(self, xb):\n        return self.network(xb)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Resnet_np = ResNet34_np().to(device)\nResnet_np.network[8], Resnet_np.network[9]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loss function\nloss_fn = nn.CrossEntropyLoss()\n\n# Set up cutom optimizer with weight decay\noptimizer = optim.Adam(Resnet_np.parameters(), lr=0.0001)\n\n\ndef acc_fn(predb, yb):\n    predb = torch.sigmoid(predb)\n    return (predb.argmax(dim=1) == yb.to(device)).float().mean()\n\nUntrained_ResNet34 = './Untrained_ResNet34_CloudSegModel.pt'\ntrain_losses, valid_losses, train_accuracies, valid_accuracies, best_acc= train(Resnet_np, RGBtrain_loader, RGBvalid_loader, \n                                             loss_fn, optimizer, acc_fn, epochs=25, save_path = Untrained_ResNet34)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_accuracies(train_accuracies, valid_accuracies)\nplt.savefig('./Accuracies_plot_Untrained_ResNet34.png')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_losses(train_losses, valid_losses)\nplt.savefig('./Losses_plot_Untrained_ResNet34.png')","metadata":{},"execution_count":null,"outputs":[]}]}